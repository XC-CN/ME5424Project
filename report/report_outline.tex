\documentclass[12pt]{ctexart}
\usepackage{geometry}
\geometry{a4paper,margin=1in}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{siunitx}
\usepackage{hyperref}
\usepackage{cite}

\title{ME5424 Course Project Report: Phase-Based Reinforcement Learning for the Eagle–Hen Chain Adversarial Task}
\author{项目组}
\date{\today}

\begin{document}
\hypersetup{pageanchor=false}
\renewcommand{\contentsname}{Table of Contents}
\renewcommand{\abstractname}{Abstract}
\renewcommand{\refname}{References}
\renewcommand{\figurename}{Figure}
\renewcommand{\tablename}{Table}
\begin{titlepage}
\centering
{\LARGE ME5424 Course Project Report: Phase-Based Reinforcement Learning for the Eagle–Hen Chain Adversarial Task\\[12pt]}
{\large Contributors: Project Team\\}
{\large Advisor: —\\}
{\large Affiliation: —\\}
{\large Submission Date: \today\\}
\vfill
\end{titlepage}
\hypersetup{pageanchor=true}

\tableofcontents
\bigskip
\newpage
\begin{abstract}
This report addresses the cold-start and convergence challenges of multi-agent adversarial reinforcement learning in physics-rich environments. We propose a curriculum-based, phase-wise training scheme: Stage 1 trains hens against a heuristic eagle, and Stage 2 freezes the hen policy while training the eagle. Under Box2D physics and Gymnasium interfaces, we adopt Stable-Baselines3 PPO with parallel sampling and TensorBoard logging to build a reproducible engineering pipeline. Our contributions include environment modeling (world, chain and joints, boundary and bounce), observation and reward design, parallel training with evaluation callbacks, and behavior visualization with metric analysis. Results show improved hen defense in Stage 1 (mean return increases, explained variance approaches 1, entropy loss decreases) and effective eagle approach/capture behavior in Stage 2 (positive potential-based rewards, sparse positive returns upon capture), with overall smooth training curves. Limitations include reward weighting and physical parameters requiring empirical tuning. Future work will consider joint fine-tuning, self-play, and extensions to more complex scenarios.
\end{abstract}

\textbf{Keywords:} Multi-agent adversarial learning; Curriculum learning; PPO; Gymnasium; Box2D

\section{Introduction}
This project focuses on multi-agent adversarial reinforcement learning under realistic physical constraints. We construct a two-dimensional task where hens guard a chain of chicks while an eagle attacks the tail, to assess stable learning under sparse objectives and strongly coupled dynamics. Early weak policies and sparse returns impede adversarial shaping; chain actions and inertia propagate via distance joints causing delay and oscillation, and boundary constraints with bounce further increase optimization difficulty. Direct joint training tends to oscillate or prematurely converge. We adopt a phase-based curriculum: Stage 1 fixes the eagle to a heuristic tail-chasing policy and trains hens to learn blocking and guarding; Stage 2 freezes the hen policy and trains the eagle to learn approach and capture against a non-trivial opponent. Implemented with Gymnasium and Box2D, and combined with viewpoint-specific observation normalization, potential-based reward shaping, and unified boundary/bounce modeling, this scheme mitigates distribution instability and exploration failures.

Regarding roles, the eagle forms a local state each step from positions of uncaptured chicks and observations of guardians/teammates, with rewards emphasizing rapid capture, boundary adherence, and penalizing repetitive circling, including penalties when repelled by hens. Hens maintain observations around a safety radius, prioritize protection of chicks within that radius, and gain higher rewards by blocking along the eagle–tail line; any capture penalizes the group to drive tight defense. Chicks continually sense the nearest hen and eagle; their policy favors moving toward protectors and away from threats, with rewards encouraging staying within the guarding radius and penalizing capture, yielding coordinated evasion. This tri-party design aligns objectives, physical constraints, and interpretable rewards, facilitating analysis and evaluation.
Our goal is to validate phase-based training for sparse-reward MARL using realistic chain dynamics and interpretable rewards, and to deliver a reproducible, evaluable pipeline with teaching value. Behaviorally, hens remain centered under chain inertia, defend the tail, and form an effective blocking line between the eagle and tail; the eagle learns side/rear detours and accelerated approach. Metric-wise, Stage 1 shows steady mean return growth, explained variance near 1, and decreasing entropy loss; Stage 2 shows increased capture success, positive approach potentials, and smooth overall loss curves. We emphasize repeatability and verifiability via fixed seeds, standardized evaluation callbacks, and visualizations, providing instructional and research value.





\section{Literature Review and Research Status}
\subsection{Current Status and Challenges}
Multi-agent reinforcement learning (MARL) for adversarial tasks often uses joint training or self-play. In sparse-reward, physics-coupled scenarios, training can oscillate, advantage estimates exhibit high variance, and mutual adaptation induces distribution shift. Mainstream approaches include:
\begin{itemize}
\item Using PPO, SAC and other on-/off-policy methods with parallel sampling and entropy regularization to mitigate exploration deficiency\cite{raffin2022stable,haarnoja2018sac}
\item Employing curriculum learning to gradually increase task difficulty\cite{bengio2009curriculum}
\item Introducing potential-based rewards, shaping, or imitation learning to stabilize training\cite{nguyen2020deep}
\end{itemize}
Limitations:
\begin{itemize}
\item Early joint training yields ineffective adversaries with near-zero returns and noisy gradients
\item Single-view observations lack consistent normalization in each agent’s body frame
\item Physics collisions and boundary constraints are not properly encoded in rewards, leading to pathological behaviors such as boundary stalling
\end{itemize}
\subsection{Multi-Agent RL and Self-Play}
Adversarial multi-agent tasks are common in robotics cooperation and game-theoretic environments. Self-play and opponent modeling are typical routes to improved robustness, but early training often suffers from sparse rewards and unstable distributions. Mutual adaptation causes distribution shift, increasing optimization difficulty.

A systematic review summarizes challenges and progress in communication, cooperation, and adversarial MARL, highlighting non-stationarity, credit assignment, and insufficient exploration as key bottlenecks requiring stable algorithms and principled task design\cite{nguyen2020deep}. In communication, differentiable end-to-end mechanisms can improve cooperation/adversarial performance under partial observability but introduce optimization instabilities, which must be controlled via regularization and curriculum strategies\cite{foerster2016learning}.

\subsection{Proximal Policy Optimization (PPO)}
PPO constrains policy update magnitude via a clipped objective, combines advantage estimation and entropy regularization, and balances stability with sample efficiency, making it a mainstream baseline for continuous control. We use SB3 with parallel environments and large batches to reduce temporal correlation, along with evaluation callbacks and TensorBoard monitoring.

Compared to other continuous control methods, DDPG trains deterministically with target networks, suiting low-dimensional action spaces but being sensitive to exploration and hyperparameters\cite{lillicrap2016ddpg}; SAC, based on maximum entropy, encourages balanced exploration and often exhibits improved robustness in complex tasks\cite{haarnoja2018sac}. PPO’s simple updates, mature implementations, and relative insensitivity to hyperparameters make it a preferred choice in engineering contexts, with reliable open-source support for reproducibility\cite{raffin2022stable}.

\subsection{Curriculum Learning}
Curriculum learning alleviates cold-start and exploration challenges by progressively increasing task difficulty. Here, Stage 1 fixes the eagle to a heuristic policy so hens can learn blocking/defense against a controlled opponent; Stage 2 freezes the hen policy and trains the eagle to learn approach/capture against a non-trivial opponent. This avoids near-zero gradients from ineffective early adversaries.

Classically, curriculum learning organizes samples/objectives in a sequence from easy to hard, improving generalization and stability\cite{bengio2009curriculum}. In adversarial MARL, phase-wise or semi-static training alleviates oscillation and maladaptation from simultaneous bilateral learning without altering the final task, and has shown effectiveness in group adversarial settings\cite{cai2023multiphase}.

\subsection{Reward Shaping and Potential Functions}
Reward shaping introduces potential functions or geometric scores to convert sparse objectives into dense, optimizable signals. We use: (1) a geometric blocking score measuring whether hens effectively position between the eagle and tail; (2) an approach potential capturing reductions in eagle–tail distance; and (3) stretch, boundary, and bounce terms to enforce physical plausibility and encourage active exploration.

Under the maximum entropy framework, entropy regularization encourages diverse policies and maintains sufficient exploration near local optima; combined with potential-based rewards, it can further improve stability and sample efficiency\cite{haarnoja2018sac}. In MARL, rewards depend on both individual and team states, so shaping must align with physical constraints to avoid unreasonable strategic biases\cite{nguyen2020deep}.

\subsection{Physics Simulation and Interpretability}
Box2D distance joints and rigid-body collisions provide interpretable dynamics for the chain and agents. Unified boundary constraints and bounce rules yield learned strategies that match intuition in visualizations (e.g., hens spreading “wings” to form a blocking band, eagle rear detours and accelerated approach), enhancing physical consistency and instructional readability.

For standardized evaluation in control tasks, the DeepMind Control Suite provides reproducible task sets and metrics, forming a basis for comparison and replication\cite{tassa2018deepmind}. We follow similar reproducibility principles: fixed seeds, standardized logging and evaluation callbacks, and unified physical parameters and interface definitions\cite{raffin2022stable}.

\subsection{Parallel Sampling and Stable Training}
Parallel environments (\texttt{SubprocVecEnv} with \texttt{n\_envs}=16) reduce single-environment temporal correlation and increase sampling throughput. Environments step asynchronously with batched rollouts aggregated to global updates; fixed seeds ensure reproducibility. An \texttt{EvalCallback} runs at a fixed frequency (e.g., every \(K\) updates), saving the best-performing checkpoints to enable phase-wise review and consistent comparisons.

Engineering practice shows that stable implementations, clear logging and evaluation protocols, and consistency with community baselines are critical to reproducible experiments\cite{raffin2022stable}. We align interfaces and settings with mainstream baselines to minimize bias from implementation differences.


\section{Project Design and Implementation}
\subsection{Requirements Analysis}
The system must support stable adversarial interactions within a 2D world boundary:
\begin{itemize}
\item Hens defend the tail and maintain stable chain motion
\item The eagle efficiently approaches and captures within a limited number of steps
\item Observations are normalized in each agent’s body frame
\item Rewards balance goal attainment with physical plausibility
\item Training supports parallel sampling and reproducibility, with evaluation and visualization
\end{itemize}
\subsection{Solution Design (Technical Approach and Rationale)}
Our pipeline centers on reproducibility: two derived training stages on a unified physical environment, parallel sampling and evaluation callbacks to ensure sampling independence and observation stability, and consistent designs for observation normalization, reward shaping, and boundary bounce to avoid redundant theory and unstable implementations. Specific algorithmic and simulation choices are detailed in subsequent subsections.

\subsection{Implementation (Workflow and Key Steps)}
\textbf{Stage 1}
\begin{itemize}
\item Initialize world and chain
\item Eagle uses a heuristic tail-chasing strategy
\item Hens learn defense and position control with PPO
\item Evaluation callback saves the best model
\end{itemize}
\textbf{Stage 2}
\begin{itemize}
\item Load and freeze the hen policy
\item Train the eagle with PPO to learn approach and capture
\item Use the evaluation callback to save the best model
\item Provide visualization scripts for behavior inspection
\end{itemize}


\subsection{Physical Environment Implementation}
Physical parameters are managed via a unified configuration, including \texttt{world\_size}, \texttt{dt}, \texttt{max\_steps}, \texttt{chain\_links}, \texttt{chain\_spacing}, \texttt{hen\_max\_speed}, \texttt{eagle\_max\_speed}, \texttt{catch\_radius}, and \texttt{block\_margin}. World construction and chain/joint setup use dedicated builders, and forces with velocity clipping are applied during action to maintain simulation stability.

\subsection{Observation and Action Spaces}
Observations are normalized in each agent’s body frame; actions are continuous 2D accelerations:
\begin{itemize}
\item Eagle observations include relative tail position/velocity, relative bearing and distance to hens/teammates, and boundary distance/normal; action \(a\in\mathbb{R}^2\) is an acceleration vector with subsequent velocity clipping and boundary bounce.
\item Hen observations include geometric relations to tail/eagle (line angle, perpendicular distance), approximate chain tension indicators, and boundary distance; actions are 2D accelerations for centering and blocking control.
\item Chick observations include relative position/velocity to the nearest hen and eagle; actions tend to move toward protectors and away from threats to reduce stretch penalty and capture risk.
\end{itemize}

\subsection{Physical Parameters and Configuration}
\begin{table}[h]
\centering
\sisetup{round-mode=places}
\begin{tabular}{ll}
\toprule
Physical Parameter & Value \\
\midrule
World half-size \texttt{world\_size} & 20.0 \\
Time step \texttt{dt} & $\frac{1}{30}$ s \\
Max steps per episode \texttt{max\_steps} & 600 \\
Chain links \texttt{chain\_links} & 7 \\
Chain spacing \texttt{chain\_spacing} & 1.0 \\
Hen max speed \texttt{hen\_max\_speed} & 9.0 \\
Eagle max speed \texttt{eagle\_max\_speed} & 36.0 \\
Capture radius \texttt{catch\_radius} & 0.7 \\
Blocking margin \texttt{block\_margin} & 2.0 \\
\bottomrule
\end{tabular}
\caption{Overview of physical environment parameters (design and implementation)}
\end{table}

The parameter choices balance physical plausibility and training stability: \texttt{world\_size} bounds exploration; \texttt{dt} sets integration fidelity; \texttt{chain\_links} and \texttt{chain\_spacing} control flexibility and oscillation; \texttt{hen\_max\_speed} and \texttt{eagle\_max\_speed} differentiate maneuverability to avoid stalemates; \texttt{catch\_radius} reflects capture locality; \texttt{block\_margin} encodes a practical blocking width. Values were validated by small grid searches to minimize excessive stretch and boundary collisions while maintaining task difficulty.

\subsection{Training Hyperparameters and Configuration}
\begin{table}[h]
\centering
\begin{tabular}{ll}
\toprule
Training Hyperparameter & Value \\
\midrule
Parallel environments \texttt{n\_envs} & 16 \\
Steps per environment \texttt{n\_steps} & 256 \\
Batch size \texttt{batch\_size} & 512 \\
Learning rate \texttt{learning\_rate} & $3\times10^{-4}$ \\
Discount factor \texttt{gamma} & 0.995 \\
Total steps \texttt{total\_steps} & 1,000,000 \\
Device \texttt{device} & auto/cpu \\
\bottomrule
\end{tabular}
\caption{Overview of PPO training hyperparameters (design and implementation)}
\end{table}

Hyperparameters were tuned for stability and throughput: \texttt{n\_envs}=16 with \texttt{SubprocVecEnv} reduces temporal correlation; \texttt{n\_steps}=256 balances return estimation and memory; \texttt{batch\_size}=512 and learning rate \(3\times10^{-4}\) follow SB3 baselines; \(\gamma=0.995\) preserves long-horizon rewards. Pilot runs monitored explained variance and policy entropy to avoid under- or over-regularization.

\subsection{Chain Dynamics and Team Coordination}
Chicks are connected via distance joints to form a flexible chain. Joint target length, frequency, and damping determine chain stiffness and oscillation amplitude. Chain structure directly impacts team coordination:
\begin{itemize}
\item \textbf{Coupling and delay}: Acceleration or turning of any chick propagates through joints to neighbors, creating temporal lag and phase differences; hens must guide stably during defense to avoid excessive stretch and sway.
\item \textbf{Shape and formation}: Motion naturally forms head–middle–tail speed/inertia gradients; the tail is easiest for the eagle to approach, so defense prioritizes tail protection and building a blocking line between eagle and tail.
\item \textbf{Constraints and coordination}: Chain length and tension constrain team maneuverability; chick policies tend to stay within the hen’s safety radius and maintain spacing from teammates, reducing stretch penalties and increasing stability.
\item \textbf{Reward coupling}: Stretch, boundary, and bounce terms simultaneously affect multiple segments’ rewards, coupling individual behavior with team outcomes and promoting coordinated evasion/guarding.
\end{itemize}

\subsection{Rewards and Collision Mechanics}
The Stage 1 hen total reward \(R_H\) is a weighted sum of:
\begin{align}
&\text{Blocking score}&& r_{\text{block}}=\max\bigl(0,\ 1-\tfrac{d(H,\overline{ET})}{a}\bigr) \tag{1}\\
&\text{Avoidance score}&& r_{\text{avoid}}=\mathbb{I}[\text{not captured}] \tag{2}\\
&\text{Stretch penalty}&& r_{\text{stretch}}=-\lambda_s\sum_{i} \bigl|\|x_i-x_{i-1}\|-\ell\bigr| \tag{3}\\
&\text{Tail distance penalty}&& r_{\text{tail}}=-\lambda_t\,\|x_T-x_H\| \tag{4}\\
&\text{Boundary penalty}&& r_{\text{bound}}=-\lambda_b\,\mathbb{I}[\text{out of bounds}] \tag{5}\\
&\text{Survival reward}&& r_{\text{survive}}=\alpha \tag{6}
\end{align}
Define \(R_H=\alpha_1 r_{\text{block}}+\alpha_2 r_{\text{avoid}}+\alpha_3 r_{\text{survive}}+r_{\text{stretch}}+r_{\text{tail}}+r_{\text{bound}}\).

Example coefficients used in experiments: \(R_H\) combines \(0.7\,r_{\text{block}}+0.3\,r_{\text{avoid}}-0.05\,\text{stretch}-0.1\,\text{tail distance}-0.2\,\text{boundary}+\text{survival bonus}\) with a small time-scaled survival bonus and a \(-200)\) penalty upon capture.

The Stage 2 eagle total reward \(R_E\) includes:
\begin{align}
&\text{Capture reward}&& r_{\text{catch}}=\kappa\,\mathbb{I}[\text{captured}] \tag{7}\\
&\text{Approach potential}&& r_{\text{pot}}=\beta\,(\|x_{T}^{t-1}-x_{E}^{t-1}\|-\|x_T^{t}-x_E^{t}\|) \tag{8}\\
&\text{Stretch suppression}&& r_{\text{stretch}}=+\eta\sum_{i}\bigl(\ell-\|x_i-x_{i-1}\|\bigr)_{+} \tag{9}\\
&\text{Boundary penalty}&& r_{\text{bound}}=-\lambda_b\,\mathbb{I}[\text{out of bounds}] \tag{10}\\
&\text{Pacing and speed}&& r_{\text{pace}}=\rho\,\|v_E\|-\rho'\,\mathbb{I}[\text{repetitive circling}] \tag{11}\\
&\text{Time pressure}&& r_{\text{time}}=-\zeta \tag{12}
\end{align}
Define \(R_E=r_{\text{catch}}+r_{\text{pot}}+r_{\text{stretch}}+r_{\text{bound}}+r_{\text{pace}}+r_{\text{time}}\).

Example coefficients used in experiments: capture \(+100\); potential term coefficient \(\beta\approx 2.0\) per unit distance reduction; stretch bonus \(\approx 0.5\) beyond a baseline; bounce penalty \(\approx -2.0\); border penalty \(\approx -5.0\); flanking and speed bonuses \(\approx 0.5\) and \(0.1\) scaled; distance pressure \(\approx -0.01\,\text{distance}\); time pressure and low-activity penalties increase gradually over the episode.

Collisions and bounce are handled uniformly: upon agent–boundary or “wing band” collisions, velocity is decomposed into normal and tangential components; the normal component is reflected with energy loss, and position is offset along the normal to avoid penetration, ensuring stable simulation and interpretability. In implementation, normal reflection uses a restitution of 0.5 and tangential scaling of 0.9; a minimum offset of 0.5 m (or 0.15 times the blocking span) prevents re-penetration and enforces separation.

\subsection{Training and Evaluation}
Stage 1 uses parallel environments (e.g., \(n\_envs=16\), \(n\_steps=256\) per env, batch size 512, learning rate \(3\times10^{-4}\), discount \(\gamma=0.995\)). Stage 2 trains the eagle under a frozen hen policy with the same settings. The evaluation callback periodically saves the best model to the output directory.

\subsection{Visualization Window}
Agent behaviors are rendered with Matplotlib visualization scripts. Role observations and rewards are summarized in the introduction.
\begin{figure}[h]
\centering
\begin{subfigure}{0.48\textwidth}
\includegraphics[width=\linewidth]{figs/stage_1/Stage2_gui.jpg}
\caption{Stage 1 behavior visualization snapshot (hen defense)}
\end{subfigure}
\begin{subfigure}{0.48\textwidth}
\includegraphics[width=\linewidth]{figs/stage_2/Stage1_gui.jpg}
\caption{Stage 2 behavior visualization snapshot (eagle approach)}
\end{subfigure}
\caption{Behavior visualization examples across stages}
\end{figure}


\section{Results and Analysis}
\subsection{Game Rules}
Within a bounded 2D world and \(\texttt{max\_steps}\) per episode: the eagle aims to capture the tail; hens guard the chain and prioritize tail protection; blocking along the eagle–tail line is rewarded; out-of-bounds causes bounce and penalties; repetitive circling is penalized. Initial states are randomized within ranges to ensure non-trivial adversaries. Rules are consistent with the introduction and design sections to avoid redundancy.
\subsection{Result Data Processing}
During training we log \texttt{ep\_rew\_mean}, losses and entropy, explained variance, etc. To improve stability and readability, we apply:
\begin{itemize}
\item Metric smoothing: exponential moving average \(m_t=(1-\beta) m_{t-1}+\beta x_t\) (e.g., \(\beta=0.1\)) to reduce high-variance fluctuations
\item Step alignment: resampling/interpolation to align events from parallel environments to a unified time step for comparable curves
\item Warm-start removal: exclude early non-stationary samples (e.g., first \(N\) evaluations) to avoid initialization bias
\item Metric normalization: zero-mean unit-variance or interval scaling for mixed-dimension indicators to enable joint visualization
\end{itemize}

\subsection{Training Curves}
\begin{figure}[h]
\centering
\begin{subfigure}{0.48\textwidth}
\includegraphics[width=\linewidth]{figs/stage_1/ep_rew_mean.png}
\caption{Stage 1 mean episode return vs. timesteps (up to 1e6)}
\end{subfigure}
\begin{subfigure}{0.48\textwidth}
\includegraphics[width=\linewidth]{figs/stage_1/explained_variance.png}
\caption{Stage 1 value function explained variance vs. timesteps}
\end{subfigure}
\begin{subfigure}{0.48\textwidth}
\includegraphics[width=\linewidth]{figs/stage_1/entropy_loss.png}
\caption{Stage 1 policy entropy vs. timesteps}
\end{subfigure}
\begin{subfigure}{0.48\textwidth}
\includegraphics[width=\linewidth]{figs/stage_1/loss.png}
\caption{Stage 1 PPO overall loss vs. timesteps}
\end{subfigure}
\caption{Stage 1 training metrics summary (mean return, explained variance, policy entropy, PPO loss)}
\end{figure}

\begin{figure}[h]
\centering
\begin{subfigure}{0.48\textwidth}
\includegraphics[width=\linewidth]{figs/stage_2/rew_mean.png}
\caption{Stage 2 mean episode return vs. timesteps (up to 1e6)}
\end{subfigure}
\begin{subfigure}{0.48\textwidth}
\includegraphics[width=\linewidth]{figs/stage_2/explained_variance.png}
\caption{Stage 2 value function explained variance vs. timesteps}
\end{subfigure}
\begin{subfigure}{0.48\textwidth}
\includegraphics[width=\linewidth]{figs/stage_2/entropy_loss.png}
\caption{Stage 2 policy entropy vs. timesteps}
\end{subfigure}
\begin{subfigure}{0.48\textwidth}
\includegraphics[width=\linewidth]{figs/stage_2/loss.png}
\caption{Stage 2 PPO overall loss vs. timesteps}
\end{subfigure}
\caption{Stage 2 training metrics summary (mean return, explained variance, policy entropy, PPO loss)}
\end{figure}

% 可视化窗口已移至“项目设计与实现”章节

%

\subsection{Results Analysis}
In Stage 1, mean return steadily increases, explained variance approaches 1, and entropy loss decreases, indicating a transition from exploration to more deterministic behavior and a well-fit value function. In Stage 2, the approach potential drives continuous reductions in eagle–tail distance, capture events yield sparse positive returns, and overall loss curves remain smooth. Visualizations show hens staying centered under chain inertia and blocking the eagle, while the eagle learns side/rear detours and accelerated approach under a frozen hen policy. Both stages achieve their expected objectives.

\section{Conclusion}
\subsection{Conclusion}
This project decomposes the cold-start challenge in adversarial MARL via curriculum learning. With viewpoint normalization, physical bounce and geometric scoring, parallel sampling, and evaluation callbacks, we achieve stable training and visual verification, delivering a reproducible pipeline and model artifacts. Innovations include frozen-opponent inference within the curriculum, geometric blocking scores, and unified bounce mechanics. Limitations include manual tuning of reward weights and physical parameters, and potential overfitting to boundaries under extreme settings. Future work: introduce joint fine-tuning and self-play, explore richer potentials or inverse RL for better generalization, extend to more complex obstacles/maps, and systematize evaluation metrics with statistical significance testing.

\section{Appendix: Running and Reproducibility}
Dependencies are installed via \texttt{pip} and \texttt{conda}. Training scripts are \texttt{python src/train\_hen.py} and \texttt{python src/train\_eagle.py}; visualization scripts are \texttt{python src/visualize\_hen\_stage1.py} and \texttt{python src/visualize\_eagle\_stage2.py}. Logs are viewed with \texttt{tensorboard --logdir results/curriculum}. Source locations are indicated throughout to ensure visibility of reproduction paths and experiment repeatability.

Entry points and structure: physical environment and curriculum logic are in \texttt{src/curriculum\_env.py}; training scripts in \texttt{src/train\_hen.py} and \texttt{src/train\_eagle.py}; visualization tools in \texttt{src/visualize\_hen\_stage1.py} and \texttt{src/visualize\_eagle\_stage2.py}.

\newpage
\nocite{schulman2017ppo,lillicrap2016ddpg,cai2023multiphase,bengio2009curriculum,foerster2016learning,nguyen2020deep,haarnoja2018sac,raffin2022stable,tassa2018deepmind}
\bibliographystyle{unsrt}
\bibliography{references}

\end{document}
